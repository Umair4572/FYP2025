# Complete Model Evaluation Guide - All 4 Models

## Overview

You now have **4 complete model notebooks**, all using the same SMOTE-balanced data pipeline:

1. ‚úÖ **Logistic Regression** - `logistic_regression_clean.ipynb`
2. ‚úÖ **Random Forest** - `random_forest_clean.ipynb`  
3. ‚úÖ **XGBoost** - `xgboost_improved_clean.ipynb`
4. ‚úÖ **Neural Network** - `neural_network_clean.ipynb`

---

## What Each Notebook Includes

All notebooks have:
- ‚úÖ Load preprocessed SMOTE data (from data pipeline)
- ‚úÖ Train model with optimal hyperparameters
- ‚úÖ Optimize classification threshold (F1-score)
- ‚úÖ Calibrate probabilities (Isotonic regression)
- ‚úÖ Evaluate on validation and test sets
- ‚úÖ Generate visualizations (ROC, PR curves, confusion matrix)
- ‚úÖ Feature importance analysis
- ‚úÖ **Complete metrics**: AUC-ROC, Precision, Recall, F1-Score, TPR, FPR, Optimal Threshold
- ‚úÖ **Comparison table** with all models

---

## How to Run

### Step 1: Ensure Data Pipeline Has Run

```bash
cd credit_risk_fyp
python -m src.data_pipeline
```

This creates processed data in `data/processed/`:
- `train_smote.csv` (364,313 samples - SMOTE balanced)
- `val.csv` (75,000 samples)
- `test.csv` (100,000 samples)

### Step 2: Run Each Model Notebook

Open in Jupyter and run all cells:

```bash
jupyter notebook credit_risk_fyp/notebooks/
```

**Run in any order:**
1. `xgboost_improved_clean.ipynb` (Already run - results available)
2. `logistic_regression_clean.ipynb`
3. `random_forest_clean.ipynb`
4. `neural_network_clean.ipynb`

Each takes 5-15 minutes depending on the model.

---

## Expected Results (Test Set)

Based on XGBoost results, here's what to expect:

| Model | AUC-ROC | Precision | Recall | F1-Score | TPR | FPR |
|-------|---------|-----------|--------|----------|-----|-----|
| **Baseline** | 0.7086 | 0.3425 | 0.5223 | 0.4138 | 0.5223 | 0.2000 |
| **XGBoost (SMOTE)** | **0.7249** | 0.3415 | **0.6269** | **0.4421** | **0.6269** | 0.3586 |
| **Logistic Reg** | ~0.68-0.70 | ~0.30-0.35 | ~0.55-0.60 | ~0.40-0.43 | ? | ? |
| **Random Forest** | ~0.70-0.73 | ~0.32-0.36 | ~0.58-0.63 | ~0.42-0.45 | ? | ? |
| **Neural Network** | ~0.70-0.74 | ~0.31-0.37 | ~0.56-0.64 | ~0.41-0.46 | ? | ? |

**Key Insight:** XGBoost currently leads with 20% recall improvement over baseline!

---

## Comparison Tables

### In Each Notebook

Each notebook's **last cell** generates a comparison table showing:
- All 4 models side-by-side
- Best performer highlighted for each metric
- Improvement percentage over baseline
- Saved to `results/all_models_comparison.csv`

### After Running All Notebooks

Update the comparison cell in `neural_network_clean.ipynb` with actual values:

```python
comparison_data = {
    'Model': ['Baseline', 'Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network'],
    'AUC-ROC': [0.7086, 0.XXXX, 0.XXXX, 0.7249, test_metrics['roc_auc']],
    # Replace 0.XXXX with actual values from notebooks
}
```

---

## Model-Specific Features

### 1. Logistic Regression
- **Fastest** to train (~2 minutes)
- **Interpretable** coefficients
- Good baseline for linear relationships
- Feature importance: Coefficient magnitudes

### 2. Random Forest
- **Robust** to overfitting
- **Feature importance** via Gini importance
- Handles non-linear relationships
- No need for feature scaling

### 3. XGBoost
- **Best performance** (proven: 0.7249 AUC)
- **Gradient boosting** power
- Handles class imbalance well
- Feature importance via gain/weight/cover

### 4. Neural Network
- **Deep learning** approach
- Can capture **complex patterns**
- Requires more data and tuning
- Best for very large datasets

---

## Files Generated

### Models
```
models/
‚îú‚îÄ‚îÄ logistic_regression_smote.pkl
‚îú‚îÄ‚îÄ random_forest_smote.pkl
‚îú‚îÄ‚îÄ xgboost_smote_improved.pkl
‚îî‚îÄ‚îÄ neural_network_smote.h5
```

### Results
```
results/
‚îú‚îÄ‚îÄ figures/                      # All plots and charts
‚îÇ   ‚îú‚îÄ‚îÄ *_roc_curve.png
‚îÇ   ‚îú‚îÄ‚îÄ *_precision_recall_curve.png
‚îÇ   ‚îú‚îÄ‚îÄ *_confusion_matrix.png
‚îÇ   ‚îú‚îÄ‚îÄ *_feature_importance.png
‚îÇ   ‚îî‚îÄ‚îÄ all_models_comprehensive_comparison.png
‚îî‚îÄ‚îÄ all_models_comparison.csv     # Comparison table
```

---

## Evaluation Metrics Explained

### AUC-ROC (Area Under ROC Curve)
- **Range**: 0.5 (random) to 1.0 (perfect)
- **Interpretation**: Overall model discrimination ability
- **Target**: > 0.70 (good), > 0.80 (excellent)

### Precision
- **Formula**: TP / (TP + FP)
- **Interpretation**: Of all predicted defaults, how many were correct?
- **Trade-off**: Higher precision ‚Üí fewer false alarms

### Recall (TPR - True Positive Rate)
- **Formula**: TP / (TP + FN)
- **Interpretation**: Of all actual defaults, how many did we catch?
- **Trade-off**: Higher recall ‚Üí catch more defaults but more false alarms

### F1-Score
- **Formula**: 2 * (Precision * Recall) / (Precision + Recall)
- **Interpretation**: Harmonic mean of precision and recall
- **Target**: Balance between precision and recall

### FPR (False Positive Rate)
- **Formula**: FP / (FP + TN)
- **Interpretation**: Of all good loans, how many did we incorrectly flag?
- **Trade-off**: Lower FPR ‚Üí fewer false alarms but might miss defaults

---

## Choosing the Best Model

Consider these factors:

### 1. **Business Priority**
- **Minimize missed defaults** ‚Üí Choose model with highest **Recall**
- **Minimize false alarms** ‚Üí Choose model with highest **Precision**
- **Balance both** ‚Üí Choose model with highest **F1-Score**
- **Best overall** ‚Üí Choose model with highest **AUC-ROC**

### 2. **Deployment Constraints**
- **Speed required** ‚Üí Logistic Regression (fastest)
- **Interpretability required** ‚Üí Logistic Regression or Random Forest
- **Best performance** ‚Üí XGBoost or Neural Network
- **Limited resources** ‚Üí Logistic Regression or Random Forest

### 3. **Current Winner** üèÜ
**XGBoost (SMOTE)** with:
- AUC-ROC: **0.7249** (+2.3% over baseline)
- Recall: **0.6269** (+20% over baseline) ‚≠ê
- F1-Score: **0.4421** (+6.8% over baseline)
- **Catches 11 more defaulters per 100!**

---

## Next Steps

1. ‚úÖ **Run all 4 notebooks** to get complete results
2. ‚úÖ **Compare metrics** in the comparison table
3. ‚úÖ **Analyze trade-offs** between precision and recall
4. ‚úÖ **Choose best model** based on business priorities
5. ‚úÖ **Document findings** for FYP report

### For FYP Report

**Key Points to Highlight:**
- Implemented 4 different ML algorithms
- Used SMOTE to handle class imbalance
- Comprehensive evaluation with 7+ metrics
- Clear model comparison with visualizations
- XGBoost achieved 20% improvement in recall
- Professional data pipeline with verification
- Demonstrated understanding of trade-offs

---

## Troubleshooting

### "Processed data not found"
```bash
cd credit_risk_fyp
python -m src.data_pipeline
```

### "TensorFlow not found" (Neural Network)
```bash
pip install tensorflow
```

### Notebooks show 0.0000 in comparison
- This means that model hasn't been run yet
- Run that specific notebook first
- Re-run the comparison cell

---

**All 4 model notebooks are ready to run!** üéâ

Start with any notebook and compare results. XGBoost is currently the leader with 0.7249 AUC-ROC!

