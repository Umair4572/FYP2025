{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Model Comparison\n",
    "\n",
    "This notebook compares all trained models and identifies the best performer based on:\n",
    "1. **TPR (True Positive Rate / Recall)** - How many actual defaults we catch\n",
    "2. **FPR (False Positive Rate)** - How many good loans we incorrectly flag as risky\n",
    "3. **Other metrics** - AUC-ROC, Precision, F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model results...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root / 'credit_risk_fyp'))\n",
    "\n",
    "from src.config import RESULTS_DIR\n",
    "\n",
    "print(\"Loading model results...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Logistic Regression loaded\n",
      "‚úì Random Forest loaded\n",
      "‚úó XGBoost not found - please train this model first\n",
      "‚úì Neural Network loaded\n",
      "\n",
      "================================================================================\n",
      "Successfully loaded 3 models\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Function to safely load model results\n",
    "def load_model_results(model_name, file_name):\n",
    "    try:\n",
    "        with open(RESULTS_DIR / file_name, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"‚úì {model_name} loaded\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚úó {model_name} not found - please train this model first\")\n",
    "        return None\n",
    "\n",
    "# Load all models\n",
    "models = {\n",
    "    'Logistic Regression': load_model_results('Logistic Regression', 'logistic_regression_metrics.pkl'),\n",
    "    'Random Forest': load_model_results('Random Forest', 'random_forest_metrics.pkl'),\n",
    "    'XGBoost': load_model_results('XGBoost', 'xgboost_metrics.pkl'),\n",
    "    'Neural Network': load_model_results('Neural Network', 'neural_network_metrics.pkl')\n",
    "}\n",
    "\n",
    "# Filter out models that weren't loaded\n",
    "models = {k: v for k, v in models.items() if v is not None}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Successfully loaded {len(models)} models\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPLETE MODEL COMPARISON\n",
      "================================================================================\n",
      "              Model  AUC-ROC  Precision  Recall (TPR)  F1-Score      FPR  Threshold\n",
      "           Baseline 0.708600   0.342500      0.522300  0.413800 0.200000   0.500000\n",
      "Logistic Regression 0.636523   0.263136      0.653555  0.375205 0.455570   0.474747\n",
      "      Random Forest 0.710550   0.328313      0.610255  0.426937 0.310782   0.393939\n",
      "     Neural Network 0.720602   0.335848      0.629823  0.438089 0.310033   0.410296\n",
      "\n",
      "‚úì Saved to: c:\\Users\\Faheem\\Desktop\\Umair FYP\\FYP2025\\credit_risk_fyp\\results\\final_model_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Baseline results (from original analysis)\n",
    "baseline = {\n",
    "    'AUC-ROC': 0.7086,\n",
    "    'Precision': 0.3425,\n",
    "    'Recall (TPR)': 0.5223,\n",
    "    'F1-Score': 0.4138,\n",
    "    'FPR': 0.2000,\n",
    "    'Threshold': 0.5000\n",
    "}\n",
    "\n",
    "# Build comparison DataFrame\n",
    "rows = []\n",
    "\n",
    "# Add baseline\n",
    "rows.append({\n",
    "    'Model': 'Baseline',\n",
    "    **baseline\n",
    "})\n",
    "\n",
    "# Add trained models\n",
    "for model_name, data in models.items():\n",
    "    tm = data['test_metrics']\n",
    "    rows.append({\n",
    "        'Model': model_name,\n",
    "        'AUC-ROC': tm['roc_auc'],\n",
    "        'Precision': tm['precision'],\n",
    "        'Recall (TPR)': tm['recall'],\n",
    "        'F1-Score': tm['f1_score'],\n",
    "        'FPR': tm['false_positive_rate'],\n",
    "        'Threshold': data['optimal_threshold']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = RESULTS_DIR / 'final_model_comparison.csv'\n",
    "comparison_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n‚úì Saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Identify Best Models by Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BEST MODELS BY METRIC\n",
      "================================================================================\n",
      "\n",
      "üéØ BEST TPR (Catch Most Defaults):\n",
      "   Model: Logistic Regression\n",
      "   TPR: 0.6536 (65.36%)\n",
      "   ‚Üí Out of 100 defaults, catches 65.4\n",
      "\n",
      "üéØ BEST FPR (Fewest False Alarms):\n",
      "   Model: Baseline\n",
      "   FPR: 0.2000 (20.00%)\n",
      "   ‚Üí Out of 100 good loans, incorrectly flags only 20.0\n",
      "\n",
      "üéØ BEST AUC-ROC (Overall Performance):\n",
      "   Model: Neural Network\n",
      "   AUC-ROC: 0.7206\n",
      "\n",
      "üéØ BEST F1-Score (Balanced Performance):\n",
      "   Model: Neural Network\n",
      "   F1-Score: 0.4381\n",
      "\n",
      "üéØ BEST Precision (Most Accurate Predictions):\n",
      "   Model: Baseline\n",
      "   Precision: 0.3425\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODELS BY METRIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# TPR: Higher is better (catch more defaults)\n",
    "best_tpr_idx = comparison_df['Recall (TPR)'].idxmax()\n",
    "print(f\"\\nüéØ BEST TPR (Catch Most Defaults):\")\n",
    "print(f\"   Model: {comparison_df.loc[best_tpr_idx, 'Model']}\")\n",
    "print(f\"   TPR: {comparison_df.loc[best_tpr_idx, 'Recall (TPR)']:.4f} ({comparison_df.loc[best_tpr_idx, 'Recall (TPR)']*100:.2f}%)\")\n",
    "print(f\"   ‚Üí Out of 100 defaults, catches {comparison_df.loc[best_tpr_idx, 'Recall (TPR)']*100:.1f}\")\n",
    "\n",
    "# FPR: Lower is better (fewer false alarms)\n",
    "best_fpr_idx = comparison_df['FPR'].idxmin()\n",
    "print(f\"\\nüéØ BEST FPR (Fewest False Alarms):\")\n",
    "print(f\"   Model: {comparison_df.loc[best_fpr_idx, 'Model']}\")\n",
    "print(f\"   FPR: {comparison_df.loc[best_fpr_idx, 'FPR']:.4f} ({comparison_df.loc[best_fpr_idx, 'FPR']*100:.2f}%)\")\n",
    "print(f\"   ‚Üí Out of 100 good loans, incorrectly flags only {comparison_df.loc[best_fpr_idx, 'FPR']*100:.1f}\")\n",
    "\n",
    "# AUC-ROC: Higher is better (overall discrimination)\n",
    "best_auc_idx = comparison_df['AUC-ROC'].idxmax()\n",
    "print(f\"\\nüéØ BEST AUC-ROC (Overall Performance):\")\n",
    "print(f\"   Model: {comparison_df.loc[best_auc_idx, 'Model']}\")\n",
    "print(f\"   AUC-ROC: {comparison_df.loc[best_auc_idx, 'AUC-ROC']:.4f}\")\n",
    "\n",
    "# F1-Score: Higher is better (balanced precision/recall)\n",
    "best_f1_idx = comparison_df['F1-Score'].idxmax()\n",
    "print(f\"\\nüéØ BEST F1-Score (Balanced Performance):\")\n",
    "print(f\"   Model: {comparison_df.loc[best_f1_idx, 'Model']}\")\n",
    "print(f\"   F1-Score: {comparison_df.loc[best_f1_idx, 'F1-Score']:.4f}\")\n",
    "\n",
    "# Precision: Higher is better (fewer false positives among predictions)\n",
    "best_precision_idx = comparison_df['Precision'].idxmax()\n",
    "print(f\"\\nüéØ BEST Precision (Most Accurate Predictions):\")\n",
    "print(f\"   Model: {comparison_df.loc[best_precision_idx, 'Model']}\")\n",
    "print(f\"   Precision: {comparison_df.loc[best_precision_idx, 'Precision']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: TPR vs FPR Trade-off Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TPR vs FPR TRADE-OFF ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Goal: Maximize TPR (catch defaults) while minimizing FPR (false alarms)\n",
      "\n",
      "Rank   Model                     TPR        FPR        TPR-FPR Score  \n",
      "--------------------------------------------------------------------------------\n",
      "1      Baseline                  0.5223     0.2000     0.3223         \n",
      "2      Neural Network            0.6298     0.3100     0.3198         \n",
      "3      Random Forest             0.6103     0.3108     0.2995         \n",
      "4      Logistic Regression       0.6536     0.4556     0.1980         \n",
      "\n",
      "üèÜ BEST OVERALL (TPR-FPR Balance): Baseline\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TPR vs FPR TRADE-OFF ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGoal: Maximize TPR (catch defaults) while minimizing FPR (false alarms)\\n\")\n",
    "\n",
    "# Calculate a combined score: TPR - FPR (higher is better)\n",
    "comparison_df['TPR-FPR Score'] = comparison_df['Recall (TPR)'] - comparison_df['FPR']\n",
    "\n",
    "# Sort by this score\n",
    "ranked = comparison_df.sort_values('TPR-FPR Score', ascending=False)\n",
    "\n",
    "print(f\"{'Rank':<6} {'Model':<25} {'TPR':<10} {'FPR':<10} {'TPR-FPR Score':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for idx, (i, row) in enumerate(ranked.iterrows(), 1):\n",
    "    print(f\"{idx:<6} {row['Model']:<25} {row['Recall (TPR)']:<10.4f} {row['FPR']:<10.4f} {row['TPR-FPR Score']:<15.4f}\")\n",
    "\n",
    "best_tradeoff_model = ranked.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ BEST OVERALL (TPR-FPR Balance): {best_tradeoff_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Final Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "üìä SUMMARY:\n",
      "   ‚Ä¢ Models evaluated: 4\n",
      "   ‚Ä¢ Best overall (AUC-ROC): Neural Network\n",
      "   ‚Ä¢ Best TPR-FPR balance: Baseline\n",
      "\n",
      "üí° USE CASE RECOMMENDATIONS:\n",
      "\n",
      "   1Ô∏è‚É£  For CATCHING MOST DEFAULTS (High TPR):\n",
      "       ‚Üí Use: Logistic Regression\n",
      "       ‚Üí TPR: 65.36% | FPR: 45.56%\n",
      "\n",
      "   2Ô∏è‚É£  For MINIMIZING FALSE ALARMS (Low FPR):\n",
      "       ‚Üí Use: Baseline\n",
      "       ‚Üí FPR: 20.00% | TPR: 52.23%\n",
      "\n",
      "   3Ô∏è‚É£  For BALANCED PERFORMANCE (Best F1):\n",
      "       ‚Üí Use: Neural Network\n",
      "       ‚Üí F1: 0.4381 | AUC-ROC: 0.7206\n",
      "\n",
      "   4Ô∏è‚É£  For OVERALL BEST (TPR-FPR Balance):\n",
      "       ‚Üí Use: Baseline\n",
      "       ‚Üí TPR-FPR Score: 0.3223\n",
      "\n",
      "================================================================================\n",
      "‚úÖ ANALYSIS COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Models evaluated: {len(comparison_df)}\")\n",
    "print(f\"   ‚Ä¢ Best overall (AUC-ROC): {comparison_df.loc[best_auc_idx, 'Model']}\")\n",
    "print(f\"   ‚Ä¢ Best TPR-FPR balance: {best_tradeoff_model}\")\n",
    "\n",
    "print(\"\\nüí° USE CASE RECOMMENDATIONS:\")\n",
    "print(f\"\\n   1Ô∏è‚É£  For CATCHING MOST DEFAULTS (High TPR):\")\n",
    "print(f\"       ‚Üí Use: {comparison_df.loc[best_tpr_idx, 'Model']}\")\n",
    "print(f\"       ‚Üí TPR: {comparison_df.loc[best_tpr_idx, 'Recall (TPR)']:.2%} | FPR: {comparison_df.loc[best_tpr_idx, 'FPR']:.2%}\")\n",
    "\n",
    "print(f\"\\n   2Ô∏è‚É£  For MINIMIZING FALSE ALARMS (Low FPR):\")\n",
    "print(f\"       ‚Üí Use: {comparison_df.loc[best_fpr_idx, 'Model']}\")\n",
    "print(f\"       ‚Üí FPR: {comparison_df.loc[best_fpr_idx, 'FPR']:.2%} | TPR: {comparison_df.loc[best_fpr_idx, 'Recall (TPR)']:.2%}\")\n",
    "\n",
    "print(f\"\\n   3Ô∏è‚É£  For BALANCED PERFORMANCE (Best F1):\")\n",
    "print(f\"       ‚Üí Use: {comparison_df.loc[best_f1_idx, 'Model']}\")\n",
    "print(f\"       ‚Üí F1: {comparison_df.loc[best_f1_idx, 'F1-Score']:.4f} | AUC-ROC: {comparison_df.loc[best_f1_idx, 'AUC-ROC']:.4f}\")\n",
    "\n",
    "print(f\"\\n   4Ô∏è‚É£  For OVERALL BEST (TPR-FPR Balance):\")\n",
    "print(f\"       ‚Üí Use: {best_tradeoff_model}\")\n",
    "best_idx = ranked.index[0]\n",
    "print(f\"       ‚Üí TPR-FPR Score: {comparison_df.loc[best_idx, 'TPR-FPR Score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
